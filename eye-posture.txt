"use client";

import React, { useEffect, useRef } from "react";
import * as poseDetection from "@tensorflow-models/pose-detection";
import * as faceLandmarksDetection from "@tensorflow-models/face-landmarks-detection";
import * as tf from "@tensorflow/tfjs";
import "@tensorflow/tfjs-backend-webgl";

const EyePostureAnalysis = ({ setFeedback }) => {
  const videoRef = useRef(null);

  useEffect(() => {
    let detector;
    let faceDetector;
    let interval;

    const setupDetector = async () => {
      await tf.setBackend("webgl");
      await tf.ready();

      // Load Pose & Face Detection Models
      detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, {
        modelType: poseDetection.movenet.modelType.MULTIPOSE_LIGHTNING,
      });

      faceDetector = await faceLandmarksDetection.createDetector(
        faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh,
        {
          runtime: "mediapipe",
          solutionPath: "https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh",
        }
      );

      // Start video stream
      const video = videoRef.current;
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      video.play();

      const detectPoses = async () => {
        if (detector && video.readyState === 4) {
          const poses = await detector.estimatePoses(video, { maxPoses: 5 });
          const faces = await faceDetector.estimateFaces(video);

          let eyeContact = "Analyzing...";
          let posture = "Analyzing...";
          let peopleCount = poses.length;

          // Posture & Eye Contact Analysis
          poses.forEach((pose) => {
            const leftEar = pose.keypoints.find((kp) => kp.name === "left_ear");
            const rightEar = pose.keypoints.find((kp) => kp.name === "right_ear");
            const nose = pose.keypoints.find((kp) => kp.name === "nose");

            if (leftEar && rightEar && nose) {
              const headTilt = Math.abs(leftEar.y - rightEar.y);
              const headTurn = Math.abs(nose.x - (leftEar.x + rightEar.x) / 2);

              posture = headTilt > 20 ? "Head Tilted" : headTurn > 30 ? "Head Turned" : "Head Aligned";
            }

            const leftEye = pose.keypoints.find((kp) => kp.name === "left_eye");
            const rightEye = pose.keypoints.find((kp) => kp.name === "right_eye");

            if (leftEye && rightEye && nose) {
              const eyeCenterX = (leftEye.x + rightEye.x) / 2;
              const eyeCenterY = (leftEye.y + rightEye.y) / 2;
              const distanceToNose = Math.sqrt(
                Math.pow(eyeCenterX - nose.x, 2) + Math.pow(eyeCenterY - nose.y, 2)
              );

              eyeContact = distanceToNose > 50 ? "Looking Away" : "Maintaining Eye Contact";
            }
          });

          // Calculate Confidence Score
          const confidence = calculateConfidenceScore(eyeContact, posture, peopleCount);

          // Update feedback
          setFeedback({
            eyeContact,
            posture,
            peopleCount,
            confidence,
          });
        }

        requestAnimationFrame(detectPoses);
      };

      detectPoses();
    };

    setupDetector();

    return () => {
      if (videoRef.current && videoRef.current.srcObject) {
        const tracks = videoRef.current.srcObject.getTracks();
        tracks.forEach((track) => track.stop());
      }
      clearInterval(interval);
    };
  }, [setFeedback]);

  // Confidence Score Formula
  const calculateConfidenceScore = (eyeContact, posture, peopleCount) => {
    // If no people detected, confidence is 0
    if (peopleCount === 0) return "0.00";

    let score = 100; // Start with full confidence

    // Eye Contact Deduction
    if (eyeContact === "Looking Away") score -= 20;

    // Posture Deduction
    if (posture === "Head Turned") score -= 30;
    else if (posture === "Head Tilted") score -= 15;

    // People Count Deduction (More people = Lower score)
    if (peopleCount === 2 || peopleCount === 3) score -= 10;
    else if (peopleCount >= 4) score -= 25;

    // Ensure score is within 0-100 range
    score = Math.max(0, score);

    return score.toFixed(2); // Rounded to 2 decimals
};

  return (
    <div className="relative">
      <video ref={videoRef} className="w-full h-auto rounded-lg" autoPlay muted playsInline></video>
    </div>
  );
};

export default EyePostureAnalysis;





---------------------------------------------------------------------------------
"use client";

import React, { useEffect, useRef } from "react";
import * as poseDetection from "@tensorflow-models/pose-detection";
import * as faceLandmarksDetection from "@tensorflow-models/face-landmarks-detection";
import * as tf from "@tensorflow/tfjs";
import "@tensorflow/tfjs-backend-webgl";

const EyePostureAnalysis = ({ setFeedback, isInterviewEnded }) => {
  const videoRef = useRef(null);

  useEffect(() => {
    let detector;
    let faceDetector;
    let interval;

    const setupDetector = async () => {
      await tf.setBackend("webgl");
      await tf.ready();

      // Load Pose & Face Detection Models
      detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, {
        modelType: poseDetection.movenet.modelType.MULTIPOSE_LIGHTNING,
      });

      faceDetector = await faceLandmarksDetection.createDetector(
        faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh,
        {
          runtime: "mediapipe",
          solutionPath: "https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh",
        }
      );

      // Start video stream
      const video = videoRef.current;
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      video.play();

      const detectPoses = async () => {
        if (detector && video.readyState === 4) {
          const poses = await detector.estimatePoses(video, { maxPoses: 5 });
          const faces = await faceDetector.estimateFaces(video);

          let eyeContact = "Analyzing...";
          let posture = "Analyzing...";
          let peopleCount = poses.length;

          // Posture & Eye Contact Analysis
          poses.forEach((pose) => {
            const leftEar = pose.keypoints.find((kp) => kp.name === "left_ear");
            const rightEar = pose.keypoints.find((kp) => kp.name === "right_ear");
            const nose = pose.keypoints.find((kp) => kp.name === "nose");

            if (leftEar && rightEar && nose) {
              const headTilt = Math.abs(leftEar.y - rightEar.y);
              const headTurn = Math.abs(nose.x - (leftEar.x + rightEar.x) / 2);

              posture = headTilt > 20 ? "Head Tilted" : headTurn > 30 ? "Head Turned" : "Head Aligned";
            }

            const leftEye = pose.keypoints.find((kp) => kp.name === "left_eye");
            const rightEye = pose.keypoints.find((kp) => kp.name === "right_eye");

            if (leftEye && rightEye && nose) {
              const eyeCenterX = (leftEye.x + rightEye.x) / 2;
              const eyeCenterY = (leftEye.y + rightEye.y) / 2;
              const distanceToNose = Math.sqrt(
                Math.pow(eyeCenterX - nose.x, 2) + Math.pow(eyeCenterY - nose.y, 2)
              );

              eyeContact = distanceToNose > 50 ? "Looking Away" : "Maintaining Eye Contact";
            }
          });

          // Calculate Confidence Score
          const confidence = calculateConfidenceScore(eyeContact, posture, peopleCount);

          // Update feedback
          setFeedback({
            eyeContact,
            posture,
            peopleCount,
            confidence,
          });
        }

        requestAnimationFrame(detectPoses);
      };

      detectPoses();
    };

    setupDetector();

    return () => {
// Cleanup: Stop the video stream
      if (videoRef.current && videoRef.current.srcObject) {
        const tracks = videoRef.current.srcObject.getTracks();
        tracks.forEach((track) => track.stop());
      }
      clearInterval(interval);
    };
  }, [setFeedback]);

  useEffect(() => {
    if (isInterviewEnded) {
      // Stop the video when the interview ends
      if (videoRef.current && videoRef.current.srcObject) {
        const tracks = videoRef.current.srcObject.getTracks();
        tracks.forEach((track) => track.stop());
      }
    }
  }, [isInterviewEnded]);

  // Confidence Score Formula
  const calculateConfidenceScore = (eyeContact, posture, peopleCount) => {
    // If no people detected, confidence is 0
  // If no people detected, confidence is 0
  if (peopleCount === 0) return "0.00";

    let score = 100; // Start with full confidence
 // Start with full confidence
 // Eye Contact Deduction// Start with full confidence

       // Eye Contact Deduction
    if (eyeContact === "Looking Away") score -= 20;
    if (posture === "Head Turned") score -= 30;
    else if (posture === "Head Tilted") score -= 15;

        if (peopleCount === 2 || peopleCount === 3) score -= 10;
    else if (peopleCount >= 4) score -= 25;

    return Math.max(0, score).toFixed(2);
};

  return (
    <div className="relative">
      <video ref={videoRef} className="w-full h-auto rounded-lg" autoPlay muted playsInline></video>
    </div>
  );
};

export default EyePostureAnalysis;

